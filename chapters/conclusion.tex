\section{Butler Framework}

We have deployed Butler in a production setting at the EMBL/EBI's Embassy Cloud in a configuration that utilizes 1500 CPUs, 6 TB RAM, 1 PB of Isilon storage accessed over NFS, and 40 TB of block-storage. Furthermore, we have built a series of workflows that facilitate the large-scale cancer genomics analyses carried out by the Germline Working Group of the Pan Cancer Analysis of Whole Genomes project, including:

\begin{itemize}
\item Germline SNV discovery
\item Germline SNV joint-genotyping
\item Germline SV genotyping
\item Variant Filtration
\item Sample submission
\end{itemize}

Using these workflows we have performed a number of analyses on a 725TB data set of 2834 cancer patients' DNA samples consuming a total of 546,552 CPU hours. Each analysis took no longer than two weeks to complete and utilized only 1.5\% - 2.2\% of the overall compute capacity for management overhead. On several occasions we were able detect large scale cluster instability and program crashes utilizing the Operational Management system and take corrective action with a minimal impact on overall cluster productivity.

We applied Butler in the context of the European Open Science Cloud pilot where Butler was selected as the Life Sciences demonstrator project. We deployed Butler to two additional cloud computing environments - Computer Canada, in Vancouver, and Cyfronet in Poland. The Compute Canada environment had 1000 cores, 4TB RAM, and 1PB disk. The Cyfronet environment had 700 cores, 2.5 TB RAM, and 200TB disk. We deployed a genomic data set of approximately 60TB of data to each of the environments and used Butler to perform a coordinated analysis on this data. The project successfully concluded in 2018.

Butler has been created to facilitate scientific analyses at scale and we have demonstrated that it is able to successfully perform at the level required for today's big data initiatives in the genomics domain. There are projects on the horizon, however, that are up to 3 orders of magnitude larger than the current biggest projects. such as the US All of Us\autocite{collins2015new} (with up to 1,000,000 genomes), and ICGC Argo. This means that in order to not have to proportionately increase the timeline for theses projects the computational infrastructure will have to be scaled up instead. It is thus imperative for Butler's continued relevance to be able to ascertain the framework's performance level at several orders of magnitude larger than the current 1500 core empirically obtained result. The most immediate opportunity to do so will come up in 2019 when the EMBL/EBI's Embassy Cloud will be upgraded to 5000 CPU cores and Butler has been invited to take part in the stress-testing of the upgraded cloud. 

It is important to grow the library of workflows that are readily available for the Butler system to make the framework more appealing to new users. The Technical Working Group of the PCAWG project is in the process of migrating all of the main computational pipelines that have been used in the project into Docker\autocite{merkel2014docker} containers. Although the workflows that have been developed for the Germline Working Group have not yet been ported to Docker, Airflow, the workflow system underlying Butler has support for running Docker containers. Thus, a key next step for growing the library of Butler workflows lies in the adaptation of the core PCAWG workflows to be able to easily run them on a Butler instance. This would allow Butler to offer a comprehensive set of next generation sequencing workflows that are used for cancer genomics analysis.

Deploying Butler to a larger variety of environments will confirm the multi-cloud purpose of the framework and allow for the development of a richer set of configuration and provisioning profiles, as necessitated by the differences between deployment environments. On the basis of the already completed analyses for the PCAWG Germline Working Group, and the EOSC Pilot project, the Butler framework has also been selected to help deliver the EuCanCan project, an international alliance between Europe and Canada to develop best-practices pipelines for clinical sequencing data analysis.

Thus, over the course of the next 12-24 months the focus of Butler development will be on supporting improved scalability, developing a richer set of computational pipelines and operating in a number of new cloud computing environments. These steps should result in a more robust, feature rich, and useful tool.

\section{Rheos Framework}

When considering the need for developing Rheos we were guided by our experience running existing bioinformatics algorithms at scale. The difficulties experienced during projects such as PCAWG demonstrate the need for a suite of genome analysis algorithms that are better suited to large scale computation that the current generation of tools. One hallmark of large distributed systems is that not all competing constraints can be ideally satisfied in such a system and tradeoffs are necessary to attain a satisfactory level of performance. In the case of genomic analysis the tradeoff is between analysis cost, time, and quality.

We designed Rheos as a flexible service oriented architecture that relies on data streaming and allows the user to trade off competing constraints in a dynamic fashion based on the data that is being observed, rather than a priori as current file-based batch processing systems. We developed the general definition of a stream-based operation and described several classes of such operations that may exist in a distributed system, along with their unique characteristics. We then applied this general framework to the specific domain of genomic variant calling. We defined a set of data types that are of interest in the context of genomic variant calling and then mapped them to a set of services that would be able to progressively elaborate sequencing read data to produce these data types as more and more data is observed. These services include:

\begin{description}
    \item [Read QC] - Services for calculating a series of quality measures such as average base quality, GC content, and nucleotide distribution, on a stream of reads.
    \item [Read Mapping] - Service for performing various read alignment operations with individual reads and read pairs, split-read alignment and alignment of candidate haplotypes to a reference genome.
    \item [Germline SNP Calling] - Services for detecting single nucleotide variants in a human genome.
    \item [SV Calling] - Services for calling structural variants in a human genome.
\end{description}

We devised new online algorithms for variant calling that are mathematically equivalent to their batch-processing counterparts in well established variant calling software. Unlike the batch algorithms, these iterative versions can be used to produce a callset at any point in time before all of the data is observed. This greatly facilitates the active tradeoff between cost, time and quality that is desired. 

We followed up the design of Rheos with a limited first implementation of a set of services necessary for performing germline SNP and deletion calling from a collection of raw reads. As part of the implementation we put in place a number of performance optimizations, including building a \emph{PartitionedReservoirSet} - a strategy for collecting micro-batches of reads that map closely together and sending them downstream in a single message. This strategy allowed us to enjoy some of the benefits of batching without giving up the dynamic nature of stream-based processing. We implemented a novel method for germline deletion calling from discordantly mapped reads by employing Kernel Density Estimation. The implementation of Rheos relies heavily on a number of open source packages, including Kafka for message queueing and Docker and Kubernetes for deployment. Using these products we were able to design the entire system as a set of Docker containers, and flexibly deploy them to the cloud into a Kubernetes managed cluster, letting us benefit from its automated resource management capabilities.

We evaluated the accuracy of the callsets produced by Rheos by comparing Rheos callsets made on chromosome 20 of the NA12878 samples from the Genome In a Bottle Consortium to callsets generated by well established variant callers such as the GATK, freebayes, and Delly, as well as comparing them to a high-confidence callset generated from multiple callers by the GIAB consortium. We found germline SNP calling to be ~98\% accurate compared to freebayes and GATK, and germline deletion calling to be ~80\% accurate compared to Delly and the GIAB call-set. Manual investigation of a sampling of false-positive and false-negative calls made by Rheos showed that true accuracy is probably underestimated by these figures as several of the calls made by Rheos but not other tools appeared to be legitimate, and several of the calls made by other tools but not Rheos appeared to be spurious. Nevertheless, there are clear opportunities for improving upon the initial implementation of the framework presented in this thesis.

A key feature of most leading variant callers is their ability to use local assembly of small genomic neighbourhoods in order to produce a set of alternative haplotypes, that are then evaluated based on the read support for each such haplotype to select the best one. This method produces better results than the simpler single-locus model used by the initial Rheos implementation and tools such as samtools because it models a region of the underlying DNA molecule that is being sequenced, and allows a more natural representation of small variants such as indels. It will be clearly desirable to incorporate local assembly into the Rheos variant calling process, although this will require significant work, as no current approaches exist for iterative genome assembly and new algorithms will need to be devised to accomplish this. Another key enhancement would be the support for more structural variant types than just deletions, and more sources of signal for structural variant calling than just insert sizes. 

A key consideration that has been left relatively unexplored in this thesis is Rheos' ability to produce a callset earlier than other tools by only considering a subset of the data. At any given point in time we would like to be able to describe the degree of confidence in each variant call that is being made, and estimate the probability that this variant call might change when new data is observed. If at some point we are able to produce high-confidence calls that have a low probability of changing, we may elect to stop seeing more data for those calls, thus reducing cost, and improving analysis time, while still meeting our quality targets.

From a technical standpoint, we want to continue testing of Rheos' scalability on large compute clusters and with larger data sets. Since the current results have been obtained from a single sample, there is still a lot of work left to do to convince the bioinformatics community that the Rheos approach will be viable for many thousands of genomes. Rheos services have been designed with scalability in mind, and for many, scaling is as easy as adding more servers to a pool. Those services that are stateful (such as the Locus Processor Service), are however inherently not horizontally scalable, and continued effort will be required to make sure that these services do not become the system bottleneck in the future.

Although Illumina sequencing technology is the clear current market leader for producing genomic data, new long read technologies from companies like Oxford Nanopore Technologies are finding increasing use, despite their high error rate\autocite{laver2015assessing}. These technologies are producing single reads that are many thousands of bases long. These reads dramatically increase scientists' ability to resolve structural variation within the genome\autocite{norris2016nanopore}, as well as aiding in the assembly of low complexity genomic regions, that have long evaded groups that build reference genomes\autocite{michael2018high}. These technologies stand to benefit significantly from the application of Rheos' streaming real-time analysis because it is possible to analyze a long read in real-time as it is being produced by the sequencer, and selectively prioritize the sequencing of molecules that exhibit characteristics that are of interest to researchers. For instance, when they align to a particular region of interest, or when they match a particular species among many possible candidate.

This work has successfully established the theoretical and experimental basis for the Rheos framework as an attractive and promising approach to large-scale analysis of genomic data that can be used to produce high quality scientific results today. At the same time, it opens up a rich set of new opportunities for future research at the intersection of genomic analysis for precision medicine and large-scale distributed systems.