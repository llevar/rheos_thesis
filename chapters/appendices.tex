\begin{appendices}
\chapter{Code Listings}
\captionof{listing} {Terraform configuration of a worker VM \label{lst:terraform_worker}}
\begin{minted}
[
breaklines=true,
linenos,
fontsize=\footnotesize,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{text}
provider "openstack" {
	user_name = "${var.user_name}"
	password = "${var.password}"
	tenant_name = "${var.tenant_name}"
	auth_url = "${var.auth_url}"
}

resource "openstack_compute_instance_v2" "worker" {
  	image_id = "${var.image_id}"
	flavor_name = "s1.massive"
	security_groups = ["internal"]
	name = "${concat("worker-", count.index)}"
	network = {
		uuid = "${var.main_network_id}"
	}
	connection {
		user = "${var.user}"
	 	key_file = "${var.key_file}"
	 	bastion_key_file = "${var.bastion_key_file}"
	 	bastion_host = "${var.bastion_host}"
	 	bastion_user = "${var.bastion_user}"
	 	agent = "true"
	 	
	}
	count = "175"
	key_pair = "${var.key_pair}"
	provisioner "remote-exec" {
		inline = [
			"sudo mv /home/centos/saltstack.repo /etc/yum.repos.d/saltstack.repo",
			"sudo yum install salt-minion -y",
			"sudo service salt-minion stop",
			"echo 'master: ${var.salt_master_ip}' | sudo tee  -a /etc/salt/minion",
			"echo 'id: ${concat("worker-", count.index)}' | sudo tee -a /etc/salt/minion",
			"echo 'roles: [worker, germline, consul-client]' | sudo tee -a /etc/salt/grains",
			"sudo hostname ${concat("worker-", count.index)}",
			"sudo service salt-minion start"
		]
	}
}
\end{minted}

\captionof{listing} {Terraform configuration of a security group \label{lst:terraform_security_group}}
\begin{minted}
[
breaklines=true,
linenos,
fontsize=\footnotesize,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{text}
resource "openstack_compute_secgroup_v2" "internal" {
	name = "internal"
	description = "Allows communication between instances"
	#SSH
	rule {
		from_port = 22
		to_port = 22
		ip_protocol = "tcp"
		self = "true"
	}
	#Saltstack
	rule {
		from_port = 4505
		to_port = 4506
		ip_protocol = "tcp"
		self = "true"
	}
}
\end{minted}

\captionof{listing} {Salt Pillar for specifying test data location. \label{lst:salt_pillar_test_data}}
\begin{minted}
[
breaklines=true,
linenos,
fontsize=\footnotesize,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{yaml}
test_data_sample_path: /shared/data/samples

test_data_base_url: http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/

test_samples:
  NA12874:
    -
      - NA12874.chrom11.ILLUMINA.bwa.CEU.low_coverage.20130415.bam
      - 88a7a346f0db1d3c14e0a300523d0243
    -
      - NA12874.chrom11.ILLUMINA.bwa.CEU.low_coverage.20130415.bam.bai
      - e61c0668bbaacdea2c66833f9e312bbb
\end{minted}

\captionof{listing} {Using Salt Mine to look up a server's IP Address. \label{lst:salt_mine}}
\begin{minted}
[
breaklines=true,
linenos,
fontsize=\footnotesize,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{text}
consul-client:
  service.running:
    - enable: True
    - watch:
      - file: /etc/opt/consul.d/*    
{%- set servers = salt['mine.get']('roles:(consul-server|consul-bootstrap)', 'network.ip_addrs', 'grain_pcre').values() %}
{%- set node_ip = salt['grains.get']('ip4_interfaces')['eth0'] %}
# Create a list of servers that can be used to join the cluster
{%- set join_server = [] %}
{%- for server in servers if server[0] != node_ip %}
{% do join_server.append(server[0]) %}
{%- endfor %}
join-cluster:
  cmd.run:
    - name: consul join {{ join_server[0] }}
    - watch:
      - service: consul-client
\end{minted}

\captionof{listing} {Using Top File to map States to Roles. \label{lst:salt_top_file}}
\begin{minted}
[
breaklines=true,
linenos,
fontsize=\footnotesize,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{yaml}
base:
  '*':
    - consul
    - dnsmasq
    - collectd
  'G@roles:monitoring-server':
    - influxdb
    - grafana 
  'G@roles:job-queue':
    - rabbitmq
\end{minted}

\captionof{listing} {Collectd configuration for metrics collection. \label{lst:telegraf_config}}
\begin{minted}
[
breaklines=true,
linenos,
fontsize=\footnotesize,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{text} 
# Read metrics about cpu usage
[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics.
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states.
  report_active = false


# Read metrics about disk usage by mount point
[[inputs.disk]]
  ## By default, telegraf gather stats for all mountpoints.
  ## Setting mountpoints will restrict the stats to the specified mountpoints.
  # mount_points = ["/"]

  ## Ignore some mountpoints by filesystem type. For example (dev)tmpfs (usually
  ## present on /run, /var/run, /dev/shm or /dev).
  ignore_fs = ["tmpfs", "devtmpfs", "devfs"]


# Read metrics about disk IO by device
[[inputs.diskio]]
  ## By default, telegraf will gather stats for all devices including
  ## disk partitions.
  ## Setting devices will restrict the stats to the specified devices.
  # devices = ["sda", "sdb"]
  ## Uncomment the following line if you need disk serial numbers.
  # skip_serial_number = false
  #
  ## On systems which support it, device metadata can be added in the form of
  ## tags.
  ## Currently only Linux is supported via udev properties. You can view
  ## available properties for a device by running:
  ## 'udevadm info -q property -n /dev/sda'
  # device_tags = ["ID_FS_TYPE", "ID_FS_USAGE"]
  #
  ## Using the same metadata source as device_tags, you can also customize the
  ## name of the device via templates.
  ## The 'name_templates' parameter is a list of templates to try and apply to
  ## the device. The template may contain variables in the form of '$PROPERTY' or
  ## '${PROPERTY}'. The first template which does not contain any variables not
  ## present for the device is used as the device name tag.
  ## The typical use case is for LVM volumes, to get the VG/LV name instead of
  ## the near-meaningless DM-0 name.
  # name_templates = ["$ID_FS_LABEL","$DM_VG_NAME/$DM_LV_NAME"]


# Get kernel statistics from /proc/stat
[[inputs.kernel]]
  # no configuration


# Read metrics about memory usage
[[inputs.mem]]
  # no configuration


# Get the number of processes and group them by status
[[inputs.processes]]
  # no configuration


# Read metrics about swap memory usage
[[inputs.swap]]
  # no configuration


# Read metrics about system load & uptime
[[inputs.system]]
  # no configuration
\end{minted}

\captionof{listing} {Filebeat Prospector configuration. \label{lst:filebeat_prospectors}}
\begin{minted}
[
breaklines=true,
linenos,
fontsize=\footnotesize,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{yaml}
filebeat:
  # List of prospectors to fetch data.
  prospectors:
    -
      paths:
        - /var/log/airflow/*/*
      document_type: airflow
    - 
      paths:
        - /var/log/messages
      document_type: syslog
    -
      paths:
        - /var/lib/pgsql/9.4/data/pg_log/*.log
      document_type: postgres  
   -
      paths:
        - /var/log/*.log
\end{minted}

\captionof{listing} {TICKscript for alerting on CPU value. \label{lst:cpu_value_tick}}
\begin{minted}
[
breaklines=true,
linenos,
fontsize=\footnotesize,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{shell}
// Parameters
var info = 70
var warn = 80
var crit = 90
var infoSig = 2.5
var warnSig = 3
var critSig = 3.5
var period = 10s
var every = 10s

// Dataframe
var data = stream
  |from()
    .database('metrics')
    .retentionPolicy('default')
    .measurement('cpu_value')
    .where(lambda: "type" == 'percent' AND "type_instance" == 'idle')
  |eval(lambda: 100 - "value")
    .as('used')
  |window()
    .period(period)
    .every(every)
  |mean('used')
    .as('stat')

// Thresholds
var alert = data
  |eval(lambda: sigma("stat"))
    .as('sigma')
    .keep()
  |alert()
    .id('{{ index .Tags "host"}}/cpu_value')
    .message('{{ .ID }}:{{ index .Fields "stat" }}')
    .info(lambda: "stat" > info OR "sigma" > infoSig)
    .warn(lambda: "stat" > warn OR "sigma" > warnSig)
    .crit(lambda: "stat" > crit OR "sigma" > critSig)

// Alert
alert
  .log('/tmp/cpu_alert_log_2.txt')
\end{minted}

\captionof{listing} {TICKscript for handling dead VMs. \label{lst:host_deadman_tick}}
\begin{minted}
[
breaklines=true,
linenos,
fontsize=\footnotesize,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{shell}
{% raw %}
var db = 'telegraf'
var rp = 'autogen'
var measurement = 'system'
var groupBy = ['host']
var whereFilter = lambda: TRUE
var period = 30s
var name = 'Host Deadman'
var idVar = name + ':{{.Group}}'
var blah = '{{index .Tags "host"}}'
var message = 'The host {{index .Tags "host"}} is offline as of {{.Time}}.'
var messageN = 'The host {{index .Tags "host"}} is back online at {{.Time}}.'
var idTag = 'alertID'
var levelTag = 'level'
var messageField = 'message'
var durationField = 'duration'
var outputDB = 'chronograf'
var outputRP = 'autogen'
var outputMeasurement = 'alerts'
var triggerType = 'deadman'
var threshold = 0.0
var data = stream
    |from()
        .database(db)
        .retentionPolicy(rp)
        .measurement(measurement)
        .groupBy(groupBy)
        .where(whereFilter)

var trigger = data
    |deadman(threshold, period)
        .stateChangesOnly()
        .message('{{ if eq .Level "CRITICAL" }}' + message + '{{else}}' + messageN + '{{end}}')
        .id(idVar)
        .idTag(idTag)
        .levelTag(levelTag)
        .messageField(messageField)
        .durationField(durationField)
        .slack()
        .channel('#embassyalerts')
{% endraw %}
        .exec('butler_healing_agent', 'relaunch-worker', '-t', '{{ pillar['terraform_files'] }}', '-s', '{{ pillar['terraform_state'] }}', '-v', '{{ pillar['terraform_vars'] }}', '-p', '{{ pillar['terraform_provider'] }}')
{% raw %}
trigger
    |eval(lambda: "emitted")
        .as('value')
        .keep('value', messageField, durationField)
    |influxDBOut()
        .create()
        .database(outputDB)
        .retentionPolicy(outputRP)
        .measurement(outputMeasurement)
        .tag('alertName', name)
        .tag('triggerType', triggerType)

trigger
    |httpOut('output')
{% endraw %}
\end{minted}

\captionof{listing} {Butler healing agent code for restarting the Airflow Scheduler. \label{lst:self_healing_airflow_scheduler_restart}}
\begin{minted}
[
breaklines=true,
linenos,
fontsize=\footnotesize,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{python}
def call_command(command, cwd=None):
  try:
      logging.debug("About to invoke command: " + command)
      my_output = check_output(command, shell=True, cwd=cwd, stderr=STDOUT)
      logging.debug("Command output is: " + my_output)
      return my_output
  except CalledProcessError as e:
      logging.error("An error occurred! Command output is: " + e.output.decode("utf-8") )
      raise
    
def is_critical(level):
  return level == "CRITICAL"

def parse_alert_data():
  return json.loads(sys.stdin.read())

def get_host_name(alert_data):
  return alert_data["data"]["series"][0]["tags"]["host"]

def restart_service(host, service_name):
  call_command("pepper {} service.restart {}".format(host, service_name), None)

def parse_args():
  my_parser = argparse.ArgumentParser()

  sub_parsers = my_parser.add_subparsers()

  common_args_parser = argparse.ArgumentParser(
      add_help=False, conflict_handler='resolve')
  
  restart_airflow_scheduler_parser = sub_parsers.add_parser(
      "restart-airflow-scheduler", parents=[common_args_parser], conflict_handler='resolve')
  restart_airflow_scheduler_parser.set_defaults(func=restart_airflow_scheduler_command)

def restart_airflow_scheduler_command(args, alert_data):
  if is_critical(alert_data["level"]):
      restart_service("-G 'roles:tracker'", "airflow-scheduler")
\end{minted}

\captionof{listing} {Butler healing agent code for relaunching a failed VM. \label{lst:self_healing_relaunch_vm}}
\begin{minted}
[
breaklines=true,
linenos,
fontsize=\footnotesize,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{python}
def call_command(command, cwd=None):
  try:
      logging.debug("About to invoke command: " + command)
      my_output = check_output(command, shell=True, cwd=cwd, stderr=STDOUT)
      logging.debug("Command output is: " + my_output)
      return my_output
  except CalledProcessError as e:
      logging.error("An error occurred! Command output is: " + e.output.decode("utf-8") )
      raise
    
def is_critical(level):
  return level == "CRITICAL"

def parse_alert_data():
  return json.loads(sys.stdin.read())

def get_host_name(alert_data):
  return alert_data["data"]["series"][0]["tags"]["host"]

def restart_service(host, service_name):
  call_command("pepper {} service.restart {}".format(host, service_name), None)

def parse_args():
  my_parser = argparse.ArgumentParser()

  sub_parsers = my_parser.add_subparsers()

  common_args_parser = argparse.ArgumentParser(
    add_help=False, conflict_handler='resolve')

  relaunch_worker_parser = sub_parsers.add_parser(
      "relaunch-worker", parents=[common_args_parser], conflict_handler='resolve')
  relaunch_worker_parser.add_argument(
      "-t", "--terraform_location", help="Location of the terraform definition files.",
      dest="terraform_location", required=True)
  relaunch_worker_parser.add_argument(
      "-s", "--terraform_state_location", help="Location of the terraform state file.",
      dest="terraform_state_location", required=True)
  relaunch_worker_parser.add_argument(
      "-v", "--terraform_var_file_location", help="Location of the terraform vars file.",
      dest="terraform_var_file_location", required=True)
  relaunch_worker_parser.add_argument(
      "-p", "--terraform_provider", help="The terraform provider to use.",
      choices = provider_list,
      dest="terraform_provider", required=True)
  relaunch_worker_parser.set_defaults(func=relaunch_worker_command)

  def is_key_present(key_data, host_name):
  parsed_key_data = json.loads(key_data)
  return_data = parsed_key_data["return"][0]["data"]["return"]
  
  if "minions" in return_data:
      return_vals = return_data["minions"]
      for val in return_vals:
          if val == host_name:
              return True
  
  return False

def locate_minon_key(host_name):
  minion_connect_try = 1
  while minion_connect_try <= MINION_CONNECT_MAX_RETRIES:
      logging.info("Attempt #{} of {} to retrieve minion key for host {} from the master.".format(minion_connect_try, MINION_CONNECT_MAX_RETRIES, host_name))
      key_data = call_command("pepper --client=wheel key.name_match match={}".format(host_name))
      logging.debug("Retrieved key data: " + key_data)
      if is_key_present(key_data, host_name):
          return True
      else:
          logging.debug("Key data for host {} not found at time {}. Sleeping for {} seconds.".format(host_name, datetime.now(), MINION_CONNECT_SLEEP_PERIOD))
          time.sleep(MINION_CONNECT_SLEEP_PERIOD)
          minion_connect_try = minion_connect_try + 1
          
  return False

def relaunch_worker_command(args, alert_data):    
  if is_critical(alert_data["level"]):
      host_name = get_host_name(alert_data)
      
      
      tf_location = args.terraform_location
      tf_state_location = args.terraform_state_location
      tf_var_file_location = args.terraform_var_file_location
      tf_resource = provider_resource_lookup[args.terraform_provider]
      worker_number = host_name.split("-")[1]
  
      call_command("pepper --client=wheel key.delete match={}".format(host_name))
      call_command("terraform taint -lock=false -state={} {}.worker.{}".format(tf_state_location, tf_resource, worker_number), tf_location)
      call_command("terraform apply -lock=false -state={} --var-file {} -auto-approve".format(tf_state_location, tf_var_file_location), tf_location)

      locate_minon_key(host_name)
                              
      call_command("pepper '*' mine.update")
      call_command("pepper {} state.apply dnsmasq".format(host_name))
      call_command("pepper {} state.apply consul".format(host_name))
      call_command("pepper {} state.highstate".format(host_name))  
\end{minted}

\captionof{listing} {Consul service definition for PostgreSQL. \label{lst:consul_postgres_service}}
\begin{minted}
[
breaklines=true,
linenos,
fontsize=\footnotesize,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{json}
{
	"service": {
		"name": "Postgresql", 
		"tags": ["postgresql"], 
		"port": 5432}
}
\end{minted}
\captionof{listing}{Source code for the freebayes workflow.\label{lst:freebayes_workflow}}
\begin{minted}
[
breaklines=true,
linenos,
fontsize=\footnotesize,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{python}
from airflow import DAG
from airflow.operators import BashOperator, PythonOperator
from datetime import datetime, timedelta

import os
import logging
from subprocess import call

import tracker.model
from tracker.model.analysis_run import *
from tracker.util.workflow_common import *


def run_freebayes(**kwargs):

    config = get_config(kwargs)
    logger.debug("Config - {}".format(config))
    
    sample = get_sample(kwargs)

    contig_name = kwargs["contig_name"]
    contig_whitelist = config.get("contig_whitelist")
    
    
    if not contig_whitelist or contig_name in contig_whitelist:

        sample_id = sample["sample_id"]
        sample_location = sample["sample_location"]

        result_path_prefix = config["results_local_path"] + "/" + sample_id

        if (not os.path.isdir(result_path_prefix)):
            logger.info(
                "Results directory {} not present, creating.".format(result_path_prefix))
            os.makedirs(result_path_prefix)

        result_filename = "{}/{}_{}.vcf".format(
            result_path_prefix, sample_id, contig_name)

        freebayes_path = config["freebayes"]["path"]
        freebayes_mode = config["freebayes"]["mode"]
        freebayes_flags = config["freebayes"]["flags"]
        
        reference_location = config["reference_location"]
        
        if freebayes_flags == None:
            freebayes_flags = ""
        
        if freebayes_mode == "discovery":
            freebayes_command = "{} -r {} -f {} {} {} > {}".\
                format(freebayes_path,
                       contig_name,
                       reference_location,
                       freebayes_flags,
                       sample_location,
                       result_filename)
        elif freebayes_mode == "regenotyping":
            variants_location = config["variants_location"]

            freebayes_command = "{} -r {} -f {} -@ {} {} {} > {}".\
                format(freebayes_path,
                       contig_name,
                       reference_location,
                       variants_location[contig_name],
                       freebayes_flags,
                       sample_location,
                       result_filename)
        else:
             raise ValueError("Unknown or missing freebayes_mode - {}".format(freebayes_mode))   

        call_command(freebayes_command, "freebayes")

        compressed_sample_filename = compress_sample(result_filename, config)
        generate_tabix(compressed_sample_filename, config)
        copy_result(compressed_sample_filename, sample_id, config)
    else:
        logger.info(
            "Contig {} is not in the contig whitelist, skipping.".format(contig_name))


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime.datetime(2020, 01, 01),
    'email': ['airflow@airflow.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG("freebayes", default_args=default_args,
          schedule_interval=None, concurrency=10000, max_active_runs=2000)


start_analysis_run_task = PythonOperator(
    task_id="start_analysis_run",
    python_callable=start_analysis_run,
    provide_context=True,
    dag=dag)


validate_sample_task = PythonOperator(
    task_id="validate_sample",
    python_callable=validate_sample,
    provide_context=True,
    dag=dag)

validate_sample_task.set_upstream(start_analysis_run_task)

complete_analysis_run_task = PythonOperator(
    task_id="complete_analysis_run",
    python_callable=complete_analysis_run,
    provide_context=True,
    dag=dag)

for contig_name in tracker.util.workflow_common.CONTIG_NAMES:
    freebayes_task = PythonOperator(
        task_id="freebayes_" + contig_name,
        python_callable=run_freebayes,
        op_kwargs={"contig_name": contig_name},
        provide_context=True,
        dag=dag)

    freebayes_task.set_upstream(validate_sample_task)

    complete_analysis_run_task.set_upstream(freebayes_task)
\end{minted}

\captionof{listing}{Saltstack state for workflow deployment.\label{lst:workflow_deployment}}
\begin{minted}
[
breaklines=true,
fontsize=\footnotesize,
linenos,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{yaml}
pcawg-germline-clone:
  git.latest:
    - rev: master
    - force_reset: True
    - name: https://github.com/llevar/pcawg-germline.git
    - target: /opt/pcawg-germline
    - submodules: True
    
/opt/airflow/dags:
  file.symlink:
    - target: /opt/pcawg-germline/workflows/
    - user: airflow
    - group: airflow
    - mode: 755
    - force: True
    - makedirs: True
 
/tmp/pcawg-germline/scripts:
  file.symlink:
    - target: /opt/pcawg-germline/scripts/
    - user: root
    - group: root
    - mode: 755
    - force: True
    - makedirs: True
\end{minted}

\captionof{listing}{Butler Analysis configuration for SNP genotyping.\label{lst:genotyping_analysis}}
\begin{minted}
[
breaklines=true,
breakanywhere=true,
fontsize=\footnotesize,
linenos,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{json}
{
	"variants_location": {
	    "1": "/freebayes.chr_1.sites.snv_indel.annot.final.vcf.gz",
	    "2": "/freebayes.chr_2.sites.snv_indel.annot.final.vcf.gz",
	    "3": "/freebayes.chr_3.sites.snv_indel.annot.final.vcf.gz",
	    "4": "/freebayes.chr_4.sites.snv_indel.annot.final.vcf.gz",
	    "5": "/freebayes.chr_5.sites.snv_indel.annot.final.vcf.gz",
	    "6": "/freebayes.chr_6.sites.snv_indel.annot.final.vcf.gz",
	    "7": "/freebayes.chr_7.sites.snv_indel.annot.final.vcf.gz",
	    "8": "/freebayes.chr_8.sites.snv_indel.annot.final.vcf.gz",
	    "9": "/freebayes.chr_8.sites.snv_indel.annot.final.vcf.gz",
	    "10": "/freebayes.chr_10.sites.snv_indel.annot.final.vcf.gz",
	    "11": "/freebayes.chr_11.sites.snv_indel.annot.final.vcf.gz",
	    "12": "/freebayes.chr_12.sites.snv_indel.annot.final.vcf.gz",
	    "13": "/freebayes.chr_13.sites.snv_indel.annot.final.vcf.gz",
	    "14": "/freebayes.chr_14.sites.snv_indel.annot.final.vcf.gz",
	    "15": "/freebayes.chr_15.sites.snv_indel.annot.final.vcf.gz",
	    "16": "/freebayes.chr_16.sites.snv_indel.annot.final.vcf.gz",
	    "17": "/freebayes.chr_17.sites.snv_indel.annot.final.vcf.gz",
	    "18": "/freebayes.chr_18.sites.snv_indel.annot.final.vcf.gz",
	    "19": "/freebayes.chr_19.sites.snv_indel.annot.final.vcf.gz",
	    "20": "/freebayes.chr_20.sites.snv_indel.annot.final.vcf.gz",
	    "21": "/freebayes.chr_21.sites.snv_indel.annot.final.vcf.gz",
	    "22": "/freebayes.chr_22.sites.snv_indel.annot.final.vcf.gz",
	    "X": "/freebayes.chr_X.sites.snv_indel.annot.final.vcf.gz",
	    "Y": "/freebayes.chr_Y.sites.snv_indel.annot.final.vcf.gz"
	},
	"results_base_path": "/shared/data/results/regenotype_freebayes_discovery/",
	"results_local_path": "/tmp/regenotype_freebayes_discovery/",
	"freebayes": {
		"mode": "regenotyping",
		"flags": "-l"
	}
}
\end{minted}


\captionof{listing}{Butler Workflow configuration for Data Submission.\label{lst:gtupload_workflow}}
\begin{minted}
[
breaklines=true,
breakanywhere=true,
fontsize=\footnotesize,
linenos,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{json}
{
	"gnos": {
		"ebi": {
			"url": "https://gtrepo-ebi.annailabs.com"
		},
		"osdc_icgc": {
			"url": "https://gtrepo-osdc-icgc.annailabs.com"
		},
		"osdc_tcga": {
			"url": "https://gtrepo-osdc-tcga.annailabs.com"
		}
	},
	"rsync": {
		"flags": "-a -v --remove-source-files"
	}
}

\end{minted}

\captionof{listing}{Butler Analysis configuration for Data Submission.\label{lst:gtupload_analysis}}
\begin{minted}
[
breaklines=true,
breakanywhere=true,
fontsize=\footnotesize,
linenos,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{json}
{
	"gnos": {
		"ebi": {
			"key_location": "/home/airflow/.ssh/sergei_pcawg_gnos_icgc.pem"
		},
		"osdc_icgc": {
			"key_location": "/home/airflow/.ssh/sergei_pcawg_gnos_icgc.pem"
		},
		"osdc_tcga": {
			"key_location": "/home/airflow/.ssh/sergei_bionimbus_gnos_may.pem"
		}
	},
	"metadata_template_location": "/opt/pcawg-germline/workflows/gtupload-workflow/analysis_template.xml",
	"submission_base_path": "/shared/data/results/freebayes_discovery_gnos_submission/",
	"destination_repo_mapping": {
		"ICGC": "ebi",
		"TCGA": "osdc_tcga"
	}
}
\end{minted}

\captionof{listing}{Python code for the run\_delly function which implements the functionality of the delly\_genotype task inside the Butler Delly Workflow.\label{lst:run_delly}}
\begin{minted}
[
breaklines=true,
breakanywhere=true,
fontsize=\footnotesize,
linenos,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{python}
def run_delly(**kwargs):

    config = get_config(kwargs)
    sample = get_sample(kwargs)

    sample_id = sample["sample_id"]
    sample_location = sample["sample_location"]

    result_path_prefix = config["results_local_path"] + "/" + sample_id

    if (not os.path.isdir(result_path_prefix)):
        logger.info(
            "Results directory {} not present, creating.".format(result_path_prefix))
        os.makedirs(result_path_prefix)

    delly_path = config["delly"]["path"]
    reference_location = config["reference_location"]
    variants_location = config["variants_location"]
    variants_type = config["variants_type"]
    exclude_template_path = config["delly"]["exclude_template_path"]

    result_filename = "{}/{}_{}.bcf".format(
        result_path_prefix, sample_id, variants_type)
    
    log_filename = "{}/{}_{}.log".format(
        result_path_prefix, sample_id, variants_type)

    delly_command = "{} call -t {} -g {} -v {} -o {} -x {} {} > {}".\
        format(delly_path,
               variants_type,
               reference_location,
               variants_location,
               result_filename,
               exclude_template_path,
               sample_location,
               log_filename)

    call_command(delly_command, "delly")

    copy_result(result_filename, sample_id, config)
\end{minted}

\captionof{listing}{Butler Delly Workflow analysis configuration to genotype deletions.\label{lst:delly_analysis}}
\begin{minted}
[
breaklines=true,
breakanywhere=true,
fontsize=\footnotesize,
linenos,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{json}
{
	"variants_location": "/delly_deletion_sites/del.sites.bcf",
	"results_base_path": "/shared/data/results/delly_germline_deletions_14_07_2016/",
	"results_local_path": "/tmp/delly_germline_deletions/",
	"variants_type": "DEL"
	
}
\end{minted}

\captionof{listing}{Example of a VCF file (from https://samtools.github.io/hts-specs/VCFv4.3.pdf).\label{lst:vcf_file}}
\begin{minted}
[
breaklines=true,
breakanywhere=true,
fontsize=\footnotesize,
linenos,
frame=lines,
framesep=2mm,
baselinestretch=1.2
]
{shell}
{
  ##fileformat=VCFv4.3
  ##fileDate=20090805
  ##source=myImputationProgramV3.1
  ##reference=file:///seq/references/1000GenomesPilot-NCBI36.fasta
  ##contig=<ID=20,length=62435964,assembly=B36,md5=f126cdf8a6e0c7f379d618ff66beb2da,species="Homo sapiens",taxonomy=x>
  ##phasing=partial
  ##INFO=<ID=NS,Number=1,Type=Integer,Description="Number of Samples With Data">
  ##INFO=<ID=DP,Number=1,Type=Integer,Description="Total Depth">
  ##INFO=<ID=AF,Number=A,Type=Float,Description="Allele Frequency">
  ##INFO=<ID=AA,Number=1,Type=String,Description="Ancestral Allele">
  ##INFO=<ID=DB,Number=0,Type=Flag,Description="dbSNP membership, build 129">
  ##INFO=<ID=H2,Number=0,Type=Flag,Description="HapMap2 membership">
  ##FILTER=<ID=q10,Description="Quality below 10">
  ##FILTER=<ID=s50,Description="Less than 50% of samples have data">
  ##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">
  ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description="Genotype Quality">
  ##FORMAT=<ID=DP,Number=1,Type=Integer,Description="Read Depth">
  ##FORMAT=<ID=HQ,Number=2,Type=Integer,Description="Haplotype Quality">
  #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA00001 NA00002 NA00003
  20 14370 rs6054257 G A 29 PASS NS=3;DP=14;AF=0.5;DB;H2 GT:GQ:DP:HQ 0|0:48:1:51,51 1|0:48:8:51,51 1/1:43:5:.,.
  20 17330 . T A 3 q10 NS=3;DP=11;AF=0.017 GT:GQ:DP:HQ 0|0:49:3:58,50 0|1:3:5:65,3 0/0:41:3
  20 1110696 rs6040355 A G,T 67 PASS NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:4
  20 1230237 . T . 47 PASS NS=3;DP=13;AA=T GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:51,51 0/0:61:2
  20 1234567 microsat1 GTC G,GTCT 50 PASS NS=3;DP=9;AA=G GT:GQ:DP 0/1:35:4 0/2:17:2 1/1:40:3
\end{minted}

\end{appendices}

